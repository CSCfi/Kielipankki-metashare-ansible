#! /usr/bin/env python3
# -*- mode: Python; -*-

# syncmeta

# This script connects to a metashare node and reads xml
# metadata and meta-metadata into variables for further processing
# Note that the http connection sets cookies which are available
# throughout the lifetime of the script.

# For accessing a metashare node see
# https://github.com/metashare/META-SHARE/wiki/META-SHARE-Harvesting-Protocol-v1.0

# Authors: Martin Matthiesen, CSC; Jussi Piitulainen, HY

# To start from scratch, set up your META-SHARE sync user, set up an
# empty current database in DATA, and update the current database:
# $ sqlite3 DATA/mdfirst.db < schema.sql
# $ (cd DATA; ln -s mdfirst.db current)
# $ ./syncmeta
# There should be a new current database named DATA/mdxxxxxx.db, for
# some xxxxxx.

from http.cookiejar import CookieJar
from http import client
from urllib.parse import urlencode
from urllib.request import build_opener, HTTPCookieProcessor, Request
from zipfile import ZipFile
from io import BytesIO, StringIO, TextIOWrapper
from time import time, strftime, gmtime
from lxml import etree
import datetime
import hashlib, json, os.path, shutil, sqlite3
from os import environ
from sys import stderr

from syncmetaconf import dbparent, oaiprefix, stemsize
from syncmetaconf import metasharenode, syncuser

def now():
    '''OAI-PMH date format, UTC with Z or so.'''
    stamp = round(time())
    ztime = strftime('%Y-%m-%dT%H:%M:%SZ', gmtime(stamp))
    return ztime, stamp

def login():
    '''Do protocol to access the META-SHARE node.'''

    client.HTTPConnection.debuglevel = 0 # set to one for more info
    
    cookieJar = CookieJar()
    opener = build_opener(HTTPCookieProcessor(cookieJar))

    def postdata(**items):
        return urlencode(items).encode('ASCII')

    # Step 1
    # get csrftoken cookie in opener/cookieJar
    # also sessionid in there I think
    if opts.verbose: print('login: opening', loginURL, 'with GET')
    opener.open(loginURL)

    # Extract that csrftoken
    token, = ( cookie.value
               for cookie in cookieJar
               if cookie.name == 'csrftoken' )

    # Step 2
    # Login. A new cookie with a new session id is returned.
    if opts.verbose: print('login: opening', loginURL, 'with POST')
    opener.open(loginURL,
                postdata(this_is_the_login_form = '1',
                         csrfmiddlewaretoken = token,
                         **syncuser))

    return opener

def fetchinventory(opener):
    # Step 3
    # get inventory. The cookie with the new session id obtained in
    # Step 2 is automatically returned.

    if opts.verbose: print('inventory: opening', inventoryURL)
    response = opener.open(inventoryURL)
    if opts.verbose: print('inventory: got status', response.status)
    zipbytes = response.read()
    inventoryStream=ZipFile(BytesIO(zipbytes)).open("inventory.json")
    return json.load(TextIOWrapper(inventoryStream))

def fetch(opener, storid, checksum):
    # Step 4
    # Get the resource data
    # load zipped resource data into zip object
    #print('opening', recordURL, '(with storage identifier)')
    response = None
    for x in 1, 2, 3:
        try:
            response = opener.open(recordURL.format(storid))
            break
        except Exception as whatever:
            print('fail: attempt', x, whatever)
            # Gives "attempt 1 <urlopen error [Errno -2] Name or
            # service not known>" sometimes, haven't seen more
            # attempts yet.

    # TODO: WHAT TO DO IF FAIL TO READ DATA?
    if not response:
        print('panic: failed to read resource data')
    elif response.status == 200:
        pass
    else:
        print('panic: non-200 record:', response.status, response.reason)
    zipbytes = response.read()
    lrDataZip=ZipFile(BytesIO(zipbytes))

    #unzip xml and storage_global.json
    documentbytes= lrDataZip.read("metadata.xml")
    storagebytes = lrDataZip.read('storage-global.json')
    storage = json.load(TextIOWrapper(BytesIO(storagebytes)))
    
    # compute the hashvalue of the returned data
    md5 = hashlib.md5()
    md5.update(documentbytes)
    md5.update(storagebytes)

    # Debug: is the checksum ok?
    if md5.hexdigest() == checksum:
        pass
    else:
        # TODO: WHAT TO DO IF THIS EVER HAPPENS?
        print("mismatch: %s :: %s" % (md5.hexdigest(), storage))

    return etree.parse(BytesIO(documentbytes)), storage

def dbinventory(cursor):
    cursor.execute('''
            select sourceid, code
            from origin natural join condition
            where sourcename = :origin
    ''', dict(origin = metasharenode))
    return dict(cursor)

def dbidentifiers(cursor):
    if opts.verbose: print('selecting database identifiers')
    cursor.execute('''
            select identifier, sourceid
            from origin
            where sourcename = :origin
    ''', dict(origin = metasharenode))
    return dict(cursor)

def synch(connection, opener, oldinv, newinv):
    # newinv is already filtered with seen suffixes
    # Fetch one resource at a time. Check hash values.
    #newinv = fetchinventory(opener)

    cursor = connection.cursor()
    # oldinv = dbinventory(cursor)

    # oldinv : storage identifier -> checksum in database
    # newinv : storage identifier -> checksum in metashare

    pmhids = dbidentifiers(cursor)
    # to ensure uniqueness of new OAI-PMH identifiers

    # previously unknown metashare items
    for stoid in newinv.keys() - oldinv.keys():
        #print('handle creation of', storid)
        doc, sto = fetch(opener, stoid, newinv[stoid])
        create(cursor, doc, sto, newinv[stoid], pmhids)

    # known but changed metashare items (all known if touching)
    for stoid in newinv.keys() & oldinv.keys():
        if opts.touch or newinv[stoid] != oldinv[stoid]:
            doc, sto = fetch(opener, stoid, newinv[stoid])
            update(cursor, doc, sto, newinv[stoid])

#    for counter, lrid in enumerate(newinv):
#        
#        doc, sto = fetch(opener, lrid, newinv[lrid])#
#
#        print(#counter, '\t',
#              'status:', sto['publication_status'],
#              'deleted' if sto['deleted'] else 'there',
#              '; modified:', sto['modified'])

    cursor.execute('''
        select min(stamp), max(stamp) from ix
    ''')
    minstamp, maxstamp = cursor.fetchone()
    cursor.execute('''
        update repository
        set minstamp = :minstamp,
            maxstamp = :maxstamp
    ''', dict(minstamp = minstamp, maxstamp = maxstamp))

    connection.commit()
    cursor.close()

def checkdeleted(doc, identifier, sto):
    '''Check the validity of a published META-SHARE record.  For
    actually deleted or currently unpublished records, return
    true. Test separately for published but invalid records.'''

    if sto['deleted']:
        if opts.verbose: print('deleted:', identifier)
        return True

    if sto['publication_status'] != 'p':
        if opts.verbose: print('not public:', identifier)
        return True

    return False

def create(cursor, doc, sto, chk, old):
    # identifier is in sto
    # this identifier is not in current
    # old are the identifiers in current
    # not storage identifiers but pmh, shDDDDDDC
    # must add new identifier to old
    stoid = sto['identifier']
    new = makeidentifier(stoid, old)
    old[new] = stoid
    opts.verbose and print('creating', new, '=>',
                           '{}...{}'.format(stoid[:4], stoid[-8:]))
    cursor.execute('''
            insert into origin(identifier, sourcename, sourceid)
            values (:newid, :source, :stoid)
    ''', dict(newid = new,
              source = metasharenode,
              stoid = stoid))
    cursor.execute('''
            insert into condition(identifier, code)
            values (:newid, :code)
    ''', dict(newid = new, code = chk))

    deleted = ( checkdeleted(doc, new, sto)
                or not checkInfo(doc, new) )
    for model in makeCore, makeOLAC, makeInfo, makeCMDI:
        data = model(new, doc, deleted)
        data['recno'] = makerecno(cursor)
        cursor.execute('''
           insert into record(recno, header, metadata, about)
           values (:recno, :header, :metadata, :about)
        ''', data)
        for spec in {'*'}:
            data['setSpec'] = spec
            cursor.execute('''
               insert into ix(identifier, metadataPrefix, setSpec,
                              stamp, recno)
               values (:identifier, :metadataPrefix, :setSpec,
                       :stamp, :recno)
            ''', data)

def makerecno(cursor):
    # get free recno
    cursor.execute('''
            select coalesce(max(recno), 0) from record
    ''')
    maxrecno, = cursor.fetchone()
    return maxrecno + 1

def makeidentifier(stoid, old):
    stem = stoid[-stemsize:] # last stemsize hex digits
    begin = len(oaiprefix)
    end = begin + stemsize
    new = ( '{}{}{}'
            .format(oaiprefix,
                    stem, sum(1 for x in old
                                if x[begin:end] == stem)) )
    return new

def update(cursor, doc, sto, chk):
    '''Updates the database for an item that has changed. Assume doc
    is deleted (not public) or valid.
    '''
    cursor.execute('''
        select identifier from origin
        where sourcename = :source and sourceid = :stoid
    ''', dict(source = metasharenode,
              stoid = sto['identifier']))
    identifier, = cursor.fetchone()
    cursor.execute('''
        update condition set code = :code
        where identifier = :identifier
    ''', dict(identifier = identifier,
              code = chk))

    deleted = ( checkdeleted(doc, identifier, sto)
                or not checkInfo(doc, identifier) )
    for model in makeInfo, makeOLAC, makeCore, makeCMDI:
        data = model(identifier, doc, deleted)
        cursor.execute('''
            select recno from ix
            where identifier = :identifier and
                  metadataPrefix = :metadataPrefix and
                  setSpec = '*'
        ''', data)
        recnos = cursor.fetchone()
        if recnos:
            sole, = recnos
            data['recno'] = sole
            if opts.verbose: print('update:', identifier, data['recno'])
            cursor.execute('''
                update record
                set header = :header,
                    metadata = :metadata,
                    about = :about
                where recno = :recno
            ''', data)
            cursor.execute('''
                update ix set stamp = :stamp
                where recno = :recno
            ''', data) # assuming sets remain as they were
        else:
            # This is likely to be an unexercised branch:
            # presumably there was no record in this format
            # even though the item was there. Panic?
            data['recno'] = makerecno(cursor)
            if opts.verbose: print('create:', identifier, data['recno'])
            cursor.execute('''
                insert into record(recno, header, metadata, about)
                values (:recno, :header, :metadata, :about)
            ''', data)
            # print('inserted', data['recno'])
            cursor.execute('''
                insert into ix(xno, identifier,
                               metadataPrefix, setSpec, stamp, recno)
                values(null, :identifier,
                       :metadataPrefix, '*', :stamp, :recno)
            ''', data) # assuming no sets

def makeheader(identifier, ztime, sets, deleted):
    '''Make the header element to be stored. (Sets are currently
    ignored. The parameter is there for possible future use.)'''
    header = ( '<header{deleted}>'
               '<identifier>{identifier}'
               '</identifier>'
               '<datestamp>{ztime}'
               '</datestamp>'
               '</header>' )
    return header.format(deleted = ( ' status="deleted"'
                                     if deleted else '' ),
                         identifier = identifier,
                         ztime = ztime)

def makeCore(new, doc, deleted):
    '''Returns the components of an OAI Dublin Core record as a dict
    to use in a database insertion statement for record or ix.'''
    if deleted:
        metadata = ''
    else:
        metadata = coretransform(doc)
        if coreschema(metadata):
            metadata = wrap(metadata)
            metadata = etree.tostring(metadata, encoding=str)
            if opts.verbose and opts.validate > 0: print('valid oai_dc:', new)
        else:
            metadata = ''
            if opts.validate > 0:
                print('invalid oai_dc:', new)
                report_invalidity(coreschema.error_log, doc)
            
    ztime, stamp = now()
    header = makeheader(new, ztime, (), metadata == '')
    return dict(identifier = new,
                metadataPrefix = 'oai_dc',
                stamp = stamp,
                header = header,
                metadata = metadata,
                about = '')

def makeOLAC(new, doc, deleted):
    '''Returns the components of an OLAC record as a dict to use in a
    database insertion statement for record or ix.'''
    if deleted:
        metadata = ''
    else:
        metadata = olactransform(doc)
        if olacschema(metadata):
            metadata = wrap(metadata)
            metadata = etree.tostring(metadata, encoding=str)
            if opts.verbose and opts.validate > 0: print('valid olac:', new)
        else:
            metadata = ''
            if opts.validate > 0:
                print('invalid olac:', new)
                report_invalidity(olacschema.error_log, doc)

    ztime, stamp = now()
    header = makeheader(new, ztime, (), metadata == '')
    return dict(identifier = new,
                metadataPrefix = 'olac',
                stamp = stamp,
                header = header,
                metadata = metadata,
                about = '')

def checkInfo(doc, identifier):
    '''True if doc validates with the META-SHARE schema, and report in
    stdout on the validity if requested by --verbose or --validate=n.
    Invalid records and records derived from them are stored as
    deleted.'''

    if infoschema(doc) and opts.validate > 0:
        if opts.verbose:
          print('valid meta:', identifier)
        return True

    if opts.validate > 0:
        print('invalid meta:', identifier)
        report_invalidity(infoschema.error_log, doc)

    return False

def makeInfo(new, doc, deleted):
    '''Returns the components of a META-SHARE ResourceInfo record as a
    dict to use in a database insertion statement for record or ix.
    Unlike derived formats, the record is assumed valid if not
    deleted; rather, checkInfo is used outside this function to mark
    invalid records as deleted.'''
    if deleted:
        metadata = ''
    else:
        metadata = infotransform(doc)
        metadata = etree.tostring(wrap(metadata), encoding=str)

    ztime, stamp = now()
    header = makeheader(new, ztime, (), metadata == '')
    return dict(identifier = new,
                metadataPrefix = 'info',
                stamp = stamp,
                header = header,
                metadata = metadata,
                about = '')

def makeCMDI(new, doc, deleted):
    '''Returns the components of a CMDI record as a dict to use in a
    database insertion statement for record or ix.'''

    # Theoretically risky to run the transforms at all if doc happens
    # to be invalid in META-SHARE but actual problems have not been
    # noticed. An alternative would be to decide the CMDI profile to
    # use based on the META-SHARE record itself, duplicating the logic
    # in the transform and subject to the same theoretical risk.

    metadata = cmdjtransform(cmditransform(doc)) # from Athens
    xsispace = 'http://www.w3.org/2001/XMLSchema-instance'
    xxx, = metadata.xpath('attribute::xsi:schemaLocation',
                          namespaces = dict(xsi = xsispace))
    # print('schema location:', xxx)
    # ...clarin.eu:cr1:p_1361876010571/xsd
    prefix = 'cmdi{}'.format(xxx[-8:-4])
    if opts.verbose: print('using', prefix,
                           'for {}...{}'.format(xxx[0:8], xxx[-10:]))

    if deleted:
        metadata = ''
    else:
        if cmdischema[prefix](metadata):
            metadata = etree.tostring(wrap(metadata), encoding=str)
            if opts.verbose and opts.validate > 0: print('valid {}:'.format(prefix), new)
        else:
            metadata = ''
            if opts.validate > 0:
                print('invalid {}:'.format(prefix), new)
                report_invalidity(cmdischema[prefix].error_log, doc)

    ztime, stamp = now()
    header = makeheader(new, ztime, (), metadata == '')
    return dict(identifier = new,
                metadataPrefix = prefix,
                stamp = stamp,
                header = header,
                metadata = metadata,
                about = '')

def report_invalidity(log, doc):
    '''Print in stdout opts.validate lines of log, after identifying
    information from doc, where log is a schema error log and doc is
    the original META-SHARE record. Print nothing if opts.validate is
    not positive (the default is 0).'''
    if opts.validate <= 0: return

    metaschema = dict(info = 'http://www.ilsp.gr/META-XMLSchema')
    for name in doc.xpath('descendant::info:resourceName/text()',
                          namespaces = metaschema):
        parts = StringIO(name).readlines() # sigh with stray newlines
        for line in [ part.rstrip() for part in parts ]:
            print('resource name:', line)
    for name in doc.xpath('descendant::info:resourceShortName/text()',
                          namespaces = metaschema):
        parts = StringIO(name).readlines() # sigh with stray newlines
        for line in [ part.rstrip() for part in parts ]:
            print('resource short name:', line)
    for name in doc.xpath('descendant::info:identifier/text()',
                          namespaces = metaschema):
        parts = StringIO(name).readlines() # sigh with stray newlines
        for line in [ part.rstrip() for part in parts ]:
            print('ID:', line)
    lines = StringIO(str(log)).readlines() # ugh
    for line in [ s.rstrip() for s in lines ][:opts.validate]:
        print('log:', line)
    if len(lines) > opts.validate: print('log: ...')


def main(see = ['']):
    current = os.path.join(dbparent, 'current')
    opener = login()
    newinv = fetchinventory(opener)

    connection = sqlite3.connect(current)
    cursor = connection.cursor()
    oldinv = dbinventory(cursor)
    connection.close()

    # checksums indicate that nothing changed --
    # now this ignores those that have vanished <-- BOTHER
    # (we are not likely to have those for quite a while -
    # - something has gone wrong in META-SHARE side if that
    # happens)
    if ( not opts.touch
         and all(newinv[stoid] == oldinv.get(stoid, '--')
                 for stoid in newinv
                 if any(stoid.endswith(six)
                        for six in see)) ):
        if opts.verbose: print('ok: up-to-date for',
                               'storage identifiers in seen suffixes:',
                               see)
        exit(0)

    if not opts.dryrun:
        # get file path based on day of week (0=Monday) This is slight overkill,
        # but easy to implement. Strictly we would need only 2 versions instead of 7.
        newdb_name = dbparent + '/newdb_' + str(datetime.datetime.today().weekday())  + '.db'
        try:
            shutil.copy(current, newdb_name)
        except shutil.SameFileError:
            pass
        synch(sqlite3.connect(newdb_name), opener, oldinv,
              { stoid : newinv[stoid]
                for stoid in newinv
                if any(stoid.endswith(six)
                       for six in see) })
        # set current -> newdb_name
        os.remove(current) # race condition!
        os.symlink(newdb_name, current)

if __name__ == '__main__':
    from optparse import OptionParser
    parser = OptionParser()
    parser.add_option('--see', action = 'store', default = '', # see all
                      help = 'suffixes of seen storage ids, comma-separated')
    parser.add_option('--verbose', action = 'store_true',
                      help = ( 'report progress in stdout; implies '
                               'at least --validate=1' ))
    parser.add_option('--validate', action = 'store', default = '0',
                      help = ( 'print in stdout the validity of each record '
                               'and also the META-SHARE names of '
                               'the resource and that many lines of the '
                               'validator error log when a record is'
                               'found invalid; default is 0' ))
    parser.add_option('--touch', action = 'store_true',
                      help = ( 'update seen records as if they were changed '
                               'or new' ))
    parser.add_option('--dryrun', action = 'store_true',
                      help = ( 'do not really change the database'
                               'to be used with --validate' ))

    opts, args = parser.parse_args()
    opts.validate = int(opts.validate)
    if opts.verbose: opts.validate = max(1, opts.validate)

    loginURL     = metasharenode + '/login/'
    inventoryURL = metasharenode + '/sync?sync_protocol=1.0'
    recordURL    = metasharenode + '/sync/{}/metadata/'

    # There will be an XSL Transform for each metadata prefix. All
    # records derived from schema-invalid META-SHARE records will be
    # served as "deleted", as will those other records that turn out
    # to be invalid according to their schema.

    cmditransform = etree.XSLT(etree.parse('xsl/meta-cmdi.xsl')) # partial!
    cmdjtransform = etree.XSLT(etree.parse('xsl/meta-cmdj.xsl')) # the rest.
    coretransform = etree.XSLT(etree.parse('xsl/meta-core.xsl'))
    infotransform = etree.XSLT(etree.parse('xsl/meta-info.xsl'))
    olactransform = etree.XSLT(etree.parse('xsl/meta-olac.xsl'))
    wrap = etree.XSLT(etree.parse('xsl/wrap.xsl'))

    environ['XML_CATALOG_FILES'] = 'xsd/catalog'
    infoschema = etree.XMLSchema(etree.parse('xsd/META-XMLSchema/UHEL/'
                                             'META-SHARE-Resource.xsd'))
    coreschema = etree.XMLSchema(etree.parse('xsd/oai/oai_dc.xsd'))
    olacschema = etree.XMLSchema(etree.parse('xsd/olac/olac.xsd'))
    cmdischema = {}
    for prefix, schema in ( ('cmdi0554', 'xsd/CMD0554.xsd'),
                            ('cmdi0571', 'xsd/CMD0571.xsd'),
                            ('cmdi2312', 'xsd/CMD2312.xsd'),
                            ('cmdi9836', 'xsd/CMD9836.xsd') ):
        cmdischema[prefix] = etree.XMLSchema(etree.parse(schema))

    main(opts.see.split(','))
